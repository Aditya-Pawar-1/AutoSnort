{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Module] Imports loaded ---\n",
      "--- File paths defined based on user-provided folder. ---\n",
      "--- Loading specific 15 features from paper's 'Top 15' model... ---\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Config --------------------------\n",
    "np.random.seed(42)\n",
    "\n",
    "BASE_PATH = r\"D:\\AutoSNortCopy\\XAI-AutoSnort\\Datasets\\CICIDS-2017\"\n",
    "OUTPUT_DIR = r\"D:\\AutoSNortCopy\\XAI-AutoSnort\\model_cic\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "FILE_PATHS = [\n",
    "    os.path.join(BASE_PATH, \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\"),\n",
    "    os.path.join(BASE_PATH, \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\"),\n",
    "    os.path.join(BASE_PATH, \"Friday-WorkingHours-Morning.pcap_ISCX.csv\"),\n",
    "    os.path.join(BASE_PATH, \"Monday-WorkingHours.pcap_ISCX.csv\"),\n",
    "    os.path.join(BASE_PATH, \"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\"),\n",
    "    os.path.join(BASE_PATH, \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\"),\n",
    "    os.path.join(BASE_PATH, \"Tuesday-WorkingHours.pcap_ISCX.csv\"),\n",
    "    os.path.join(BASE_PATH, \"Wednesday-workingHours.pcap_ISCX.csv\")\n",
    "]\n",
    "\n",
    "REQ_COLS = [\n",
    "    'Packet Length Std', 'Total Length of Bwd Packets', 'Subflow Bwd Bytes',\n",
    "    'Destination Port', 'Packet Length Variance', 'Bwd Packet Length Mean',\n",
    "    'Avg Bwd Segment Size', 'Bwd Packet Length Max', 'Init_Win_bytes_backward',\n",
    "    'Total Length of Fwd Packets', 'Subflow Fwd Bytes', 'Init_Win_bytes_forward',\n",
    "    'Average Packet Size', 'Packet Length Mean', 'Max Packet Length', 'Label'\n",
    "]\n",
    "REQ_COLS = [c.strip() for c in REQ_COLS]\n",
    "FEATURE_NAMES = [c for c in REQ_COLS if c != \"Label\"]\n",
    "\n",
    "\n",
    "print(\"--- [Module] Imports loaded ---\")\n",
    "print(\"--- File paths defined based on user-provided folder. ---\")\n",
    "print(f\"--- Loading specific {len(FEATURE_NAMES)} features from paper's 'Top 15' model... ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and concatenating 8 CSV files... ---\n",
      "--- Data loaded. Shape: (2830743, 16). Time: 16.75s ---\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Load --------------------------\n",
    "print(\"--- Loading and concatenating 8 CSV files... ---\")\n",
    "t0 = time.time()\n",
    "df_list = []\n",
    "for f in FILE_PATHS:\n",
    "    part = pd.read_csv(\n",
    "        f, encoding=\"latin1\", low_memory=False,\n",
    "        usecols=lambda col: col.strip() in REQ_COLS\n",
    "    )\n",
    "    df_list.append(part)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df[REQ_COLS]\n",
    "print(f\"--- Data loaded. Shape: {df.shape}. Time: {time.time()-t0:.2f}s ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting preprocessing pipeline... ---\n",
      "--- Applying custom normalization (paper's method)... ---\n",
      "--- Dropped 1903008 duplicate rows; kept 927735. ---\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Labels --------------------------\n",
    "print(\"--- Starting preprocessing pipeline... ---\")\n",
    "\n",
    "label_map = {\n",
    "    'DoS GoldenEye': 'Dos/Ddos', 'DoS Hulk': 'Dos/Ddos',\n",
    "    'DoS Slowhttptest': 'Dos/Ddos', 'DoS slowloris': 'Dos/Ddos',\n",
    "    'Heartbleed': 'Dos/Ddos', 'DDoS': 'Dos/Ddos',\n",
    "    'FTP-Patator': 'Brute Force', 'SSH-Patator': 'Brute Force',\n",
    "    'Web Attack - Brute Force': 'Web Attack',\n",
    "    'Web Attack - Sql Injection': 'Web Attack',\n",
    "    'Web Attack - XSS': 'Web Attack',\n",
    "    'Web Attack \\x96 Brute Force': 'Web Attack',\n",
    "    'Web Attack \\x96 Sql Injection': 'Web Attack',\n",
    "    'Web Attack \\x96 XSS': 'Web Attack'\n",
    "}\n",
    "y_raw = df['Label'].replace(label_map)\n",
    "X_raw = df.drop('Label', axis=1)\n",
    "\n",
    "# -------------------------- Normalize (paper's per-column max) --------------------------\n",
    "print(\"--- Applying custom normalization (paper's method)... ---\")\n",
    "X_scaled = X_raw.copy()\n",
    "scaler_values = {}\n",
    "for col in X_scaled.columns:\n",
    "    X_scaled[col] = pd.to_numeric(X_scaled[col], errors='coerce')\n",
    "    max_val = X_scaled[col].abs().max()\n",
    "    if (max_val == 0) or pd.isna(max_val):\n",
    "        X_scaled[col] = 0.0\n",
    "        max_val = 0.0\n",
    "    else:\n",
    "        X_scaled[col] = X_scaled[col] / max_val\n",
    "    scaler_values[col] = float(max_val)\n",
    "\n",
    "# Clean + dedupe (on normalized)\n",
    "df_processed = X_scaled.assign(Label=y_raw)\n",
    "df_processed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_processed.fillna(0, inplace=True)\n",
    "\n",
    "before = len(df_processed)\n",
    "df_processed.drop_duplicates(inplace=True)\n",
    "after = len(df_processed)\n",
    "print(f\"--- Dropped {before-after} duplicate rows; kept {after}. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Saved feature_names_cic.pkl & scaler_values_cic.pkl ---\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Persist Artifacts --------------------------\n",
    "with open(os.path.join(OUTPUT_DIR, \"feature_names_cic.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(FEATURE_NAMES, f)\n",
    "with open(os.path.join(OUTPUT_DIR, \"scaler_values_cic.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(scaler_values, f)\n",
    "print(\"--- Saved feature_names_cic.pkl & scaler_values_cic.pkl ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Label classes: ['BENIGN', 'Bot', 'Brute Force', 'Dos/Ddos', 'Infiltration', 'PortScan', 'Web Attack'] ---\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Encode labels --------------------------\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(df_processed['Label'].values)\n",
    "with open(os.path.join(OUTPUT_DIR, \"encoder_cic.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "with open(os.path.join(OUTPUT_DIR, \"label_value_to_name_cic.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({i: c for i, c in enumerate(le.classes_)}, f)\n",
    "print(f\"--- Label classes: {list(le.classes_)} ---\")\n",
    "\n",
    "X_final = df_processed.drop('Label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Splitting into training and testing sets (70/30)... ---\n",
      "--- X_train: (649414, 15), X_test: (278321, 15) ---\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Split --------------------------\n",
    "print(\"--- Splitting into training and testing sets (70/30)... ---\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_enc, test_size=0.30, random_state=42, stratify=y_enc\n",
    ")\n",
    "print(f\"--- X_train: {X_train.shape}, X_test: {X_test.shape} ---\")\n",
    "\n",
    "# Align raw test rows with same indices (for RAW CSV)\n",
    "X_raw_reset = X_raw.reset_index(drop=True)\n",
    "X_test_raw = X_raw_reset.loc[X_test.index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Defining & training RandomForest... ---\n",
      "--- Model trained in 17.10s ---\n",
      "--- Model saved: random_forest_model_cic.pkl ---\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Train --------------------------\n",
    "print(\"--- Defining & training RandomForest... ---\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=10, min_samples_split=2,\n",
    "    random_state=42, n_jobs=-1, class_weight='balanced'\n",
    ")\n",
    "t0 = time.time()\n",
    "rf.fit(X_train, y_train)\n",
    "print(f\"--- Model trained in {time.time()-t0:.2f}s ---\")\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"random_forest_model_cic.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(rf, f)\n",
    "print(\"--- Model saved: random_forest_model_cic.pkl ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating model on test set... ---\n",
      "\n",
      "--- [Report] Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN     0.9998    0.9857    0.9927    255132\n",
      "         Bot     0.0756    0.9830    0.1405       235\n",
      " Brute Force     0.9905    0.9858    0.9882       212\n",
      "    Dos/Ddos     0.9977    0.9979    0.9978     22019\n",
      "Infiltration     1.0000    0.8182    0.9000        11\n",
      "    PortScan     0.6817    0.9896    0.8073       671\n",
      "  Web Attack     0.0504    0.6341    0.0934        41\n",
      "\n",
      "    accuracy                         0.9866    278321\n",
      "   macro avg     0.6851    0.9135    0.7028    278321\n",
      "weighted avg     0.9979    0.9866    0.9918    278321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Evaluate --------------------------\n",
    "print(\"--- Evaluating model on test set... ---\")\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "y_test_labels = le.inverse_transform(y_test)\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "labels = le.classes_\n",
    "\n",
    "print(\"\\n--- [Report] Classification Report ---\")\n",
    "report_str = classification_report(y_test_labels, y_pred_labels, labels=labels, digits=4)\n",
    "print(report_str)\n",
    "\n",
    "# Save classification report as CSV (per-class)\n",
    "prec, rec, f1, sup = precision_recall_fscore_support(\n",
    "    y_test_labels, y_pred_labels, labels=labels, zero_division=0\n",
    ")\n",
    "cls_report_df = pd.DataFrame({\n",
    "    \"label\": labels,\n",
    "    \"precision\": prec,\n",
    "    \"recall\": rec,\n",
    "    \"f1\": f1,\n",
    "    \"support\": sup.astype(int)\n",
    "})\n",
    "cls_report_df.to_csv(os.path.join(OUTPUT_DIR, \"classification_report_per_class.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [Report] Per-Class Analysis (TP, FP, FN, TN, Acc, TPR, TNR, BAcc) ---\n",
      "\n",
      "Class: BENIGN\n",
      "  TP=251479  FP=54  FN=3653  TN=23135\n",
      "  Classwise-Accuracy(one-vs-rest)=0.9867\n",
      "  TPR/Recall=0.9857  TNR/Specificity=0.9977  Balanced-Acc=0.9917\n",
      "\n",
      "Class: Bot\n",
      "  TP=231  FP=2823  FN=4  TN=275263\n",
      "  Classwise-Accuracy(one-vs-rest)=0.9898\n",
      "  TPR/Recall=0.9830  TNR/Specificity=0.9898  Balanced-Acc=0.9864\n",
      "\n",
      "Class: Brute Force\n",
      "  TP=209  FP=2  FN=3  TN=278107\n",
      "  Classwise-Accuracy(one-vs-rest)=1.0000\n",
      "  TPR/Recall=0.9858  TNR/Specificity=1.0000  Balanced-Acc=0.9929\n",
      "\n",
      "Class: Dos/Ddos\n",
      "  TP=21973  FP=51  FN=46  TN=256251\n",
      "  Classwise-Accuracy(one-vs-rest)=0.9997\n",
      "  TPR/Recall=0.9979  TNR/Specificity=0.9998  Balanced-Acc=0.9989\n",
      "\n",
      "Class: Infiltration\n",
      "  TP=9  FP=0  FN=2  TN=278310\n",
      "  Classwise-Accuracy(one-vs-rest)=1.0000\n",
      "  TPR/Recall=0.8182  TNR/Specificity=1.0000  Balanced-Acc=0.9091\n",
      "\n",
      "Class: PortScan\n",
      "  TP=664  FP=310  FN=7  TN=277340\n",
      "  Classwise-Accuracy(one-vs-rest)=0.9989\n",
      "  TPR/Recall=0.9896  TNR/Specificity=0.9989  Balanced-Acc=0.9942\n",
      "\n",
      "Class: Web Attack\n",
      "  TP=26  FP=490  FN=15  TN=277790\n",
      "  Classwise-Accuracy(one-vs-rest)=0.9982\n",
      "  TPR/Recall=0.6341  TNR/Specificity=0.9982  Balanced-Acc=0.8162\n",
      "\n",
      "--- [Report] Overall Accuracy: 98.6598% ---\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Class-wise metrics (TP/FP/FN/TN etc.) --------------------------\n",
    "cm = confusion_matrix(y_test_labels, y_pred_labels, labels=labels)\n",
    "total = cm.sum()\n",
    "\n",
    "rows = []\n",
    "print(\"\\n--- [Report] Per-Class Analysis (TP, FP, FN, TN, Acc, TPR, TNR, BAcc) ---\")\n",
    "for i, lbl in enumerate(labels):\n",
    "    tp = int(cm[i, i])\n",
    "    fp = int(cm[:, i].sum() - tp)\n",
    "    fn = int(cm[i, :].sum() - tp)\n",
    "    tn = int(total - (tp + fp + fn))\n",
    "    tpr = (tp / (tp + fn)) if (tp + fn) > 0 else 0.0  # recall\n",
    "    tnr = (tn / (tn + fp)) if (tn + fp) > 0 else 0.0  # specificity\n",
    "    acc_ovr = (tp + tn) / total if total > 0 else 0.0\n",
    "    bacc = (tpr + tnr) / 2.0\n",
    "\n",
    "    print(f\"\\nClass: {lbl}\")\n",
    "    print(f\"  TP={tp}  FP={fp}  FN={fn}  TN={tn}\")\n",
    "    print(f\"  Classwise-Accuracy(one-vs-rest)={acc_ovr:.4f}\")\n",
    "    print(f\"  TPR/Recall={tpr:.4f}  TNR/Specificity={tnr:.4f}  Balanced-Acc={bacc:.4f}\")\n",
    "\n",
    "    rows.append(OrderedDict(\n",
    "        label=lbl, TP=tp, FP=fp, FN=fn, TN=tn,\n",
    "        classwise_accuracy=acc_ovr, recall=tpr, specificity=tnr, balanced_accuracy=bacc\n",
    "    ))\n",
    "\n",
    "per_class_df = pd.DataFrame(rows)\n",
    "per_class_df.to_csv(os.path.join(OUTPUT_DIR, \"metrics_per_class.csv\"), index=False)\n",
    "\n",
    "overall_acc = accuracy_score(y_test_labels, y_pred_labels)\n",
    "print(f\"\\n--- [Report] Overall Accuracy: {overall_acc*100:.4f}% ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [Report] Overall Model Performance Metrics ---\n",
      "Weighted Precision: 0.9979\n",
      "Weighted Recall:    0.9866\n",
      "Weighted F1-score:  0.9918\n",
      "Macro Precision:    0.6851\n",
      "Macro Recall:       0.9135\n",
      "Macro F1-score:     0.7028\n",
      "--- Overall metrics saved -> overall_metrics_cic.csv ---\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Overall Precision / Recall / F1 --------------------------\n",
    "overall_prec_weight, overall_rec_weight, overall_f1_weight, _ = precision_recall_fscore_support(\n",
    "    y_test_labels, y_pred_labels, average='weighted'\n",
    ")\n",
    "\n",
    "macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(\n",
    "    y_test_labels, y_pred_labels, average='macro'\n",
    ")\n",
    "\n",
    "print(\"\\n--- [Report] Overall Model Performance Metrics ---\")\n",
    "print(f\"Weighted Precision: {overall_prec_weight:.4f}\")\n",
    "print(f\"Weighted Recall:    {overall_rec_weight:.4f}\")\n",
    "print(f\"Weighted F1-score:  {overall_f1_weight:.4f}\")\n",
    "print(f\"Macro Precision:    {macro_prec:.4f}\")\n",
    "print(f\"Macro Recall:       {macro_rec:.4f}\")\n",
    "print(f\"Macro F1-score:     {macro_f1:.4f}\")\n",
    "\n",
    "overall_metrics_df = pd.DataFrame([{\n",
    "    \"accuracy\": overall_acc,\n",
    "    \"precision_weighted\": overall_prec_weight,\n",
    "    \"recall_weighted\": overall_rec_weight,\n",
    "    \"f1_weighted\": overall_f1_weight,\n",
    "    \"precision_macro\": macro_prec,\n",
    "    \"recall_macro\": macro_rec,\n",
    "    \"f1_macro\": macro_f1\n",
    "}])\n",
    "\n",
    "overall_metrics_df.to_csv(os.path.join(OUTPUT_DIR, \"overall_metrics_cic.csv\"), index=False)\n",
    "print(f\"--- Overall metrics saved -> overall_metrics_cic.csv ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saved normalized test set -> D:\\AutoSNortCopy\\XAI-AutoSnort\\model_cic\\test_set_with_readable_labels.csv\n",
      "--- Saved RAW (unnormalized) test set -> D:\\AutoSNortCopy\\XAI-AutoSnort\\model_cic\\test_set_raw_values.csv\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Save test CSVs --------------------------\n",
    "norm_path = os.path.join(OUTPUT_DIR, \"test_set_with_readable_labels.csv\")\n",
    "raw_path = os.path.join(OUTPUT_DIR, \"test_set_raw_values.csv\")\n",
    "\n",
    "test_norm = X_test.copy().reset_index(drop=True)\n",
    "test_norm[\"Label\"] = y_test_labels\n",
    "test_norm.to_csv(norm_path, index=False)\n",
    "\n",
    "test_raw = X_test_raw.copy()\n",
    "test_raw[\"Label\"] = y_test_labels\n",
    "test_raw.to_csv(raw_path, index=False)\n",
    "\n",
    "print(f\"\\n--- Saved normalized test set -> {norm_path}\")\n",
    "print(f\"--- Saved RAW (unnormalized) test set -> {raw_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- Confusion matrix PNG --------------------------\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Blues')\n",
    "plt.title('Confusion Matrix for CIC-IDS 2017')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "cm_path = os.path.join(OUTPUT_DIR, \"confusion_matrix_cicids.png\")\n",
    "plt.savefig(cm_path, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Confusion matrix saved -> D:\\AutoSNortCopy\\XAI-AutoSnort\\model_cic\\confusion_matrix_cicids.png\n",
      "--- Demo samples saved -> D:\\AutoSNortCopy\\XAI-AutoSnort\\model_cic\\demo_samples_raw_by_class.csv\n",
      "\n",
      "--- [Script] All tasks complete. ---\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Demo samples (raw medians per class) --------------------------\n",
    "# Use raw feature medians of each class to create ready-to-use demo rows\n",
    "df_with_label_raw = X_raw.copy()\n",
    "df_with_label_raw[\"Label\"] = y_raw.values\n",
    "demo_rows = (\n",
    "    df_with_label_raw.groupby(\"Label\")[FEATURE_NAMES]\n",
    "    .median(numeric_only=True)\n",
    "    .reset_index()\n",
    ")\n",
    "demo_csv = os.path.join(OUTPUT_DIR, \"demo_samples_raw_by_class.csv\")\n",
    "demo_rows.to_csv(demo_csv, index=False)\n",
    "\n",
    "# Input schema for UI/batch validators\n",
    "schema = {name: \"number\" for name in FEATURE_NAMES}\n",
    "with open(os.path.join(OUTPUT_DIR, \"feature_input_schema.json\"), \"w\") as f:\n",
    "    json.dump(schema, f, indent=2)\n",
    "\n",
    "print(f\"--- Confusion matrix saved -> {cm_path}\")\n",
    "print(f\"--- Demo samples saved -> {demo_csv}\")\n",
    "print(\"\\n--- [Script] All tasks complete. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoSnort (.venv)",
   "language": "python",
   "name": "autosnort-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
